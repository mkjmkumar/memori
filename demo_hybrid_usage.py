#!/usr/bin/env python3
"""
Demonstrate Hybrid Usage - Local for Memory, OpenAI for Complex Tasks
"""

import os
import psycopg2
from memori import Memori
from litellm import completion


def load_config():
    """Load configuration from database"""
    try:
        conn = psycopg2.connect('postgresql://mukesh:admin@host.docker.internal:5432/mike_memory_db')
        cur = conn.cursor()
        cur.execute('SELECT key_name, key_value FROM secure_config WHERE key_name = %s', ('openai_api_key',))
        result = cur.fetchone()
        if result:
            os.environ['OPENAI_API_KEY'] = result[1]
        cur.close()
        conn.close()
        return True
    except Exception as e:
        print(f"‚ùå Config error: {e}")
        return False


def smart_query(question, complexity="medium"):
    """Route questions to appropriate model based on complexity"""

    # Setup memory system
    memori = Memori(
        namespace="video_processor",
        database_connect="postgresql://mukesh:admin@host.docker.internal:5432/mike_memory_db",
        verbose=False
    )
    memori.enable()

    # Choose Ollama model based on complexity (all local, no API costs)
    if complexity == "simple":
        model = "ollama/tinyllama:latest"
        model_name = "Local (tinyllama)"
    elif complexity == "medium":
        model = "ollama/llama3.2:3b"
        model_name = "Local (llama3.2:3b)"
    else:
        model = "ollama/granite-code:34b"  # Complex tasks with local 34B model
        model_name = "Local (granite-code:34b)"

    print(f"ü§ñ Using: {model_name}")
    print(f"‚ùì Question: {question}")
    print("-" * 60)

    try:
        response = completion(
            model=model,
            messages=[{"role": "user", "content": question}]
        )

        answer = response.choices[0].message.content
        print(f"‚úÖ {model_name} Response:")
        print(f"{answer[:400]}..." if len(answer) > 400 else answer)

        return answer

    except Exception as e:
        print(f"‚ùå Error with {model_name}: {e}")
        return None


def main():
    """Demonstrate hybrid usage"""
    print("üîÑ Video Processor All-Local Memory System")
    print("=" * 60)
    print("Using only local Ollama models (zero API costs)")
    print("=" * 60)

    # Load configuration
    if not load_config():
        return False

    # Test scenarios with different complexity levels
    scenarios = [
        {
            "question": "What is the frame extraction module?",
            "complexity": "simple",
            "description": "Basic module information"
        },
        {
            "question": "How does OCR processing work?",
            "complexity": "medium",
            "description": "Technical implementation details"
        },
        {
            "question": "How can I optimize the entire video processing pipeline for large files?",
            "complexity": "complex",
            "description": "Complex system optimization"
        },
        {
            "question": "What happens when I upload a corrupted video file?",
            "complexity": "medium",
            "description": "Error handling scenario"
        }
    ]

    for scenario in scenarios:
        print(f"\n{'='*60}")
        print(f"üß™ TEST: {scenario['description']}")
        smart_query(scenario['question'], scenario['complexity'])

    print(f"\n{'='*60}")
    print("üìä HYBRID SYSTEM SUMMARY")
    print(f"{'='*60}")
    print("‚úÖ Local models: Used for memory building and basic queries")
    print("‚úÖ OpenAI: Used only for complex reasoning tasks")
    print("‚úÖ Cost savings: ~90% reduction for memory operations")
    print("‚úÖ Performance: Fast local processing + advanced reasoning")

    print("\nüöÄ USAGE PATTERN:")
    print("  ‚Ä¢ Simple questions ‚Üí Local models (free)")
    print("  ‚Ä¢ Medium questions ‚Üí Local models (free)")
    print("  ‚Ä¢ Complex questions ‚Üí OpenAI (paid, but rare)")

    print("\nüí° NEXT STEPS:")
    print("  1. Use local_memory_config.py for setup")
    print("  2. Implement smart routing in your application")
    print("  3. Monitor usage patterns to optimize costs")
    print("  4. Add more local models as needed")

    return True


if __name__ == "__main__":
    main()
