#!/usr/bin/env python3
"""
Video Processor Memory - Practical Demonstration
Shows how memory works in real scenarios vs direct LLM
"""

import os
import asyncio
from memori import Memori
from litellm import completion


def setup_memory():
    """Setup memory system with database configuration"""
    print("ðŸ§  Setting up Video Processor Memory System...")

    # Load configuration from database
    import psycopg2
    conn = psycopg2.connect('postgresql://mukesh:admin@host.docker.internal:5432/mike_memory_db')
    cur = conn.cursor()

    cur.execute('SELECT key_name, key_value FROM secure_config WHERE key_name = %s', ('openai_api_key',))
    result = cur.fetchone()
    if result:
        os.environ['OPENAI_API_KEY'] = result[1]

    cur.close()
    conn.close()

    # Initialize memory
    memori = Memori(
        namespace="video_processor",
        database_connect="postgresql://mukesh:admin@host.docker.internal:5432/mike_memory_db",
        conscious_ingest=True,  # Learn from conversations
        auto_ingest=True,       # Provide context automatically
        verbose=False
    )

    memori.enable()
    print("âœ… Memory system enabled and ready")
    return memori


def test_scenario(question, context_description):
    """Test a specific scenario with and without memory"""
    print(f"\n{'='*60}")
    print(f"ðŸ§ª TEST: {context_description}")
    print(f"â“ Question: {question}")
    print(f"{'='*60}")

    # Test 1: Direct LLM (no memory context)
    print("\nðŸ“ 1. DIRECT LLM RESPONSE (No Memory Context):")
    print("-" * 50)

    try:
        direct_response = completion(
            model="gpt-4o",
            messages=[{"role": "user", "content": question}],
            max_tokens=300
        )

        direct_answer = direct_response.choices[0].message.content
        print(f"âœ… Direct LLM Answer:\n{direct_answer}")

    except Exception as e:
        direct_answer = f"Error: {e}"
        print(f"âŒ Direct LLM Error: {e}")

    # Test 2: LLM with Memory Context
    print("\nðŸ“š 2. LLM WITH MEMORY CONTEXT (Video Processor Knowledge):")
    print("-" * 50)

    try:
        # This will automatically include memory context
        memory_response = completion(
            model="gpt-4o",
            messages=[{"role": "user", "content": question}],
            max_tokens=300
        )

        memory_answer = memory_response.choices[0].message.content
        print(f"âœ… Memory-Enhanced Answer:\n{memory_answer}")

    except Exception as e:
        memory_answer = f"Error: {e}"
        print(f"âŒ Memory-Enhanced Error: {e}")

    return direct_answer, memory_answer


async def demonstrate_memory_difference():
    """Demonstrate the difference between direct LLM and memory-enhanced responses"""

    print("ðŸŽ¥ Video Processor Memory System - Practical Demonstration")
    print("=" * 80)
    print("This demo shows how Memori enhances LLM responses with Video Processor knowledge")
    print("=" * 80)

    # Setup memory system
    setup_memory()

    # Test scenarios that show memory value
    test_scenarios = [
        {
            "question": "How does video processing work in the system?",
            "context": "New developer wants to understand the workflow"
        },
        {
            "question": "Why are my OCR results poor?",
            "context": "Troubleshooting video quality issues"
        },
        {
            "question": "How can I optimize processing for large video files?",
            "context": "Performance optimization question"
        },
        {
            "question": "How do the different modules communicate with each other?",
            "context": "Understanding system architecture"
        },
        {
            "question": "What happens if I upload a corrupted video file?",
            "context": "Error handling scenario"
        }
    ]

    results = []

    for scenario in test_scenarios:
        direct, memory = test_scenario(scenario["question"], scenario["context"])
        results.append({
            "question": scenario["question"],
            "direct": direct,
            "memory": memory
        })

    # Summary comparison
    print(f"\n{'='*80}")
    print("ðŸ“Š COMPARISON SUMMARY")
    print(f"{'='*80}")

    print("\nðŸ” KEY DIFFERENCES:")
    print("  â€¢ Direct LLM: General responses based only on training data")
    print("  â€¢ Memory-Enhanced: Specific responses using your Video Processor knowledge")
    print("  â€¢ Context Injection: Memory automatically adds relevant technical details")
    print("  â€¢ Accuracy: Memory responses are tailored to your specific system")

    print("\nðŸŽ¯ MEMORY VALUE DEMONSTRATION:")
    print("  1. Direct LLM might give generic video processing advice")
    print("  2. Memory-Enhanced LLM provides specific guidance about YOUR system")
    print("  3. Context includes your 7-stage pipeline, module relationships, etc.")

    print("\nðŸš€ HOW TO USE IN PRACTICE:")
    print("  Simply use any LLM as normal - Memori works automatically!")
    print("  Example:")
    print("  ```python")
    print("  from litellm import completion")
    print("  response = completion(model='gpt-4o', messages=[...])")
    print("  # Memori automatically injects Video Processor context!")
    print("  ```")

    print(f"\n{'='*80}")
    print("âœ… DEMONSTRATION COMPLETE")
    print("Your Video Processor memory system is working and enhancing LLM responses!")
    print(f"{'='*80}")


def show_usage_examples():
    """Show practical usage examples"""

    print("\nðŸ“‹ PRACTICAL USAGE EXAMPLES")
    print("=" * 50)

    examples = [
        {
            "title": "Development Question",
            "code": """
from litellm import completion

# Ask about your video processor
response = completion(
    model="gpt-4o",
    messages=[{
        "role": "user",
        "content": "How do I add support for a new video format?"
    }]
)
# Memory automatically provides context about video_input module
""",
            "memory_context": "Discusses video_input validation, frame_extraction compatibility, OCR processing requirements"
        },
        {
            "title": "Troubleshooting",
            "code": """
from litellm import completion

# Get specific troubleshooting help
response = completion(
    model="gpt-4o",
    messages=[{
        "role": "user",
        "content": "OCR quality is poor, what should I check?"
    }]
)
# Memory provides specific guidance about quality thresholds, preprocessing
""",
            "memory_context": "Explains frame quality requirements (brightness > 0.7), suggests preprocessing filters, references OCR module configuration"
        },
        {
            "title": "Performance Optimization",
            "code": """
from litellm import completion

# Get system-specific optimization advice
response = completion(
    model="gpt-4o",
    messages=[{
        "role": "user",
        "content": "How can I speed up processing for large videos?"
    }]
)
# Memory suggests parallel processing, MinIO optimization, batch processing
""",
            "memory_context": "Recommends enabling parallel processing (max 8 workers), optimizing frame extraction quality thresholds, using MinIO storage efficiently"
        }
    ]

    for i, example in enumerate(examples, 1):
        print(f"\n{i}. {example['title']}:")
        print("   Code:")
        print(f"   {example['code'].strip()}")
        print(f"   Memory Context: {example['memory_context']}")


if __name__ == "__main__":
    # Run demonstration
    asyncio.run(demonstrate_memory_difference())

    # Show usage examples
    show_usage_examples()
